{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "En este notebook vamos a hacer un clasificador de correos _spam_ utilizando el algoritmo **Naive Bayes**. Para entrenar nuestro modelo vamos a usar los datos que encontramos [en el siguiente link](https://spamassassin.apache.org/old/publiccorpus/). Este es un _dataset_ de correos _spam_ y correos _ham_ (correo deseado).\n",
    "\n",
    "Primero vamos a hacer una función que tome un mensaje y obtenga las palabras en él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de', 'ejemplo', 'es', 'este', 'hola', 'mensaje', 'un'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()                         # Pasar a minúsculas\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)  # Extraer las palabras con expresiones regulares\n",
    "    return set(all_words)                       # Con un Set no tenemos duplicados\n",
    "\n",
    "tokenize(\"Hola! este es un mensaje de ejemplo :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ponemos atención, también nos elimina los caracteres especiales. Podemos hacer esto gracias al uso de expresiones regulares. Ahora necesitamos crear una clase que represente a los mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mail():\n",
    "    def __init__(self, content, is_spam):\n",
    "        self.content = content\n",
    "        self.is_spam = is_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y después de esto podemos definir nuestro clasificador de Bayes. Sin embargo, vamos a hacer una modificación respecto a lo que vimos en clases. En este caso vamos a predecir el evento $S$: el mensaje es _spam_. Vamos a suponer que la probabilidad de que un mensaje sea _spam_ o no lo sea es la misma, 0.5. Luego formaremos un conjunto gigante con todo el universo de palabras, y así, tendremos el evento $X_i$ que significa si la palabra $i$ de nuestro vocabulario aparece en un correo.\n",
    "\n",
    "Así, si tenemos $n$ palabras, lo que queremos calcular es $P(S | X_1, \\dots, X_n)$. Entonces para determinar si un correo es _spam_, tendremos que multiplicar las probabilidades $P(X_i | S)$ y $P(X_i | ¬S)$. Como se ve, debemos pararnos en cada palabra de nuestro universo, viendo si esta está o no en el correo, y buscando la probabilidad asociada.\n",
    "\n",
    "Aquí vienen las modificaciones:\n",
    "\n",
    "- Lo primero es que en general **no nos gusta** multiplicar números pequeños. Así que transformaremos las multiplicaciones en sumas gracias a la función logaritmo.\n",
    "- Lo segundo es que podríamos pensar que P(X_i | S) lo podemos calcular como la fracción de mensajes _spam_ que contienen a la palabra $i$. Pero si tenemos una palabra que no aparece en ningún mensaje _spam_, vamos a hacer que para cualquier correo con esa palabra asignemos la probabilidad 0 de que sea spam (al multiplicar hacemos todo 0). Por esto, hacemos el siguiente truco:\n",
    "\n",
    "$$\n",
    "P(X_i | S) = \\frac{(k + \\text{número de spams que contienen la palabra}\\ i)}{2k + \\text{número de spams}}\n",
    "$$\n",
    "\n",
    "Donde $k$ es un número que vamos a escoger a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict # Diccionario con valores default\n",
    "import math\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k  # smoothing factor\n",
    "\n",
    "        self.tokens = set()\n",
    "        self.token_spam_counts = defaultdict(int)\n",
    "        self.token_ham_counts = defaultdict(int)\n",
    "        self.spam_messages = 0\n",
    "        self.ham_messages = 0\n",
    "\n",
    "    # Función para entrenar\n",
    "    def train(self, messages):\n",
    "        for message in messages:\n",
    "            # Increment message counts\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            # Increment word counts\n",
    "            for token in tokenize(message.content):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "    # Función que calcula las probabilidades asociadas a cada palabra\n",
    "    # Si la palabra no está usaremos (1 - la probabilidad)\n",
    "    def probabilities(self, token):\n",
    "        # Retorna P(token | spam) y P(token | not spam)\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "\n",
    "    # Función para predecir un texto\n",
    "    def predict(self, text):\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = 0.0\n",
    "        log_prob_if_ham = 0.0\n",
    "\n",
    "        # Vemos todo nuestro vocabulario\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self.probabilities(token)\n",
    "\n",
    "            # Si el token aparece en el mensaje sumamos el log de la probabilidad asociada\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # En otro caso, sumamos el log de la probabilidad de no haberlo visto: log(1 - probabilidad de ver el mensaje)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la clase anterior, `predict` nos va a arrojar una probabilidad. Un correo será _spam_ si la probabilidad es mayor o igual a 0.5. Es buena idea que te detengas a pensar por qué esto es así. Ahora vamos a probar nuestros modelos con unos mensajes de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8928571428571429"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    Mail(\"compra bitcoin\", is_spam=True),\n",
    "    Mail(\"bitcoin a la baja\", is_spam=True),\n",
    "    Mail(\"compra pan\", is_spam=False),\n",
    "    Mail(\"no soy spam\", is_spam=False)\n",
    "]\n",
    "\n",
    "model = NaiveBayesClassifier(k=0.5)\n",
    "model.train(messages)\n",
    "\n",
    "model.predict(\"has escuchado de bitcoin?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando el dataset\n",
    "\n",
    "Ahora vamos a considerar los correos del _link_ que presentamos más arriba. Estos están en la carpeta _mails_ junto a este notebook. Vamos a presentarte el código que te ayudará a leerlos. En este ejemplo vamos a trabajar solo con el _Subject_ del correo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Función para hacer split de un dataset\n",
    "def split_data(data, perc):\n",
    "    data = data[:]                    # Copiamos el dataset\n",
    "    random.shuffle(data)              # porque shuffle modifica la lista, aquí la ordenamos de forma aleatoria\n",
    "    cut = int(len(data) * perc)       # Vemos hasta que posición tomamos\n",
    "    return data[:cut], data[cut:]     # Retornamos las dos partes\n",
    "\n",
    "PATH_MAILS = 'mails/*/*'\n",
    "\n",
    "data = []\n",
    "\n",
    "import glob, re\n",
    "\n",
    "# Recorremos todos los archivos\n",
    "for filename in glob.glob(PATH_MAILS):\n",
    "    # Checkeamos si es spam o no\n",
    "    is_spam = 'ham' not in filename\n",
    "    \n",
    "    # Ignoramos los posibles errores al abrir un archivo\n",
    "    with open(filename, errors='ignore') as mail_file:\n",
    "        for line in mail_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                # Hacemos strip desde la izquierda\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append(Mail(subject, is_spam))\n",
    "                # Después de añadir el subject terminamos\n",
    "                break\n",
    "                \n",
    "# Generamos dos listas de mensajes\n",
    "# Una de entrenamiento y otra de prueba\n",
    "train_mails, test_mails = split_data(data, 0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
